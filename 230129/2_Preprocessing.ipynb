{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f476138c",
   "metadata": {},
   "source": [
    "# 2_EDA (3) + Preprocessing  \n",
    "references :   \n",
    "https://heytech.tistory.com/401  \n",
    "https://velog.io/@ljs7463/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%ACNLP-%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%A0%84%EC%B2%98%EB%A6%AC%EB%B2%A1%ED%84%B0%ED%99%94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea552235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296e6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab\n",
    "m = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a844abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/kimch/Desktop/NLP_news_topic_classification/train_data.csv\")\n",
    "test = pd.read_csv(\"C:/Users/kimch/Desktop/NLP_news_topic_classification/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee97dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "itsci = train[train[\"topic_idx\"]==0]\n",
    "econ = train[train[\"topic_idx\"]==1]\n",
    "society = train[train[\"topic_idx\"]==2]\n",
    "lifestyle = train[train[\"topic_idx\"]==3]\n",
    "world = train[train[\"topic_idx\"]==4]\n",
    "sports = train[train[\"topic_idx\"]==5]\n",
    "politic = train[train[\"topic_idx\"]==6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17798224",
   "metadata": {},
   "source": [
    "단어 치환 references :   \n",
    "https://dacon.io/en/competitions/official/235747/codeshare/3071?page=1&dtype=recent  \n",
    "https://dacon.io/en/competitions/official/235747/codeshare/3050?page=1&dtype=recent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8954318",
   "metadata": {},
   "source": [
    "## 한자/특수문자 치환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b8c0e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hanja\n",
      "  Using cached hanja-0.13.3-py3-none-any.whl\n",
      "Collecting pytest-cov\n",
      "  Using cached pytest_cov-4.0.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pytest in c:\\programdata\\anaconda3\\lib\\site-packages (from hanja) (6.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\_yaml\\\\__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -li5 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -bconvert (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -arkupsafe (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyyaml==5.1.2\n",
      "  Using cached PyYAML-5.1.2-cp39-cp39-win_amd64.whl\n",
      "Collecting coveralls\n",
      "  Using cached coveralls-3.3.1-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from coveralls->hanja) (2.26.0)\n",
      "Requirement already satisfied: coverage!=6.0.*,!=6.1,!=6.1.1,<7.0,>=4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from coveralls->hanja) (6.5.0)\n",
      "Requirement already satisfied: docopt>=0.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from coveralls->hanja) (0.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=1.0.0->coveralls->hanja) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=1.0.0->coveralls->hanja) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=1.0.0->coveralls->hanja) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=1.0.0->coveralls->hanja) (2.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (21.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (1.10.0)\n",
      "Requirement already satisfied: toml in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->hanja) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->pytest->hanja) (3.0.4)\n",
      "Requirement already satisfied: tomli in c:\\programdata\\anaconda3\\lib\\site-packages (from coverage!=6.0.*,!=6.1,!=6.1.1,<7.0,>=4.1->coveralls->hanja) (2.0.1)\n",
      "Installing collected packages: pyyaml, pytest-cov, coveralls, hanja\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n"
     ]
    }
   ],
   "source": [
    "!pip install hanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b094977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import hanja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeacee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한자, 특수문자\n",
    "# 코드공유한 것 보면 한자들을 일일이 찾아서 친 것 같은데 한자만 추출할 수는 없는지!!\n",
    "# -> + 한자가 1글자로 인식됨, 글자 수 1개로 추출한 후 한자만 가져옴\n",
    "# 일단 아무리 찾아봐도 잘 모르겠어서 코드공유한 것 참고함\n",
    "# hanja 라이브러리가 계속 에러가 나고 코드공유에 따르면 성능이 그리좋지 않다고 하니 고민해봐야할듯함, + 이것보다 훨씬 많은 한자 있음\n",
    "# 하다보니 한자 앞에 음 적어놓은 경우도 꽤많음 ex) 지知\n",
    "\n",
    "def chinese_etc(x):\n",
    "    x = re.sub('→', '에서', x)\n",
    "    x = re.sub('↑', '증가', x)\n",
    "    x = re.sub('↓', '감소', x)\n",
    "    x = re.sub('美', '미국', x)\n",
    "    x = re.sub('韓', '한국', x)\n",
    "    x = re.sub('日', '일본', x)\n",
    "    x = re.sub('中', '중국', x)\n",
    "    x = re.sub('英', '영국', x)\n",
    "    x = re.sub('獨', '독일', x)\n",
    "    x = re.sub(\"伊\", \"이탈리아\", x)  \n",
    "    x = re.sub(\"佛\", \"프랑스\", x)\n",
    "    x = re.sub('北', '북한', x)\n",
    "    x = re.sub(\"亞\", \"아시아\", x)\n",
    "    x = re.sub(\"印\", \"인도\", x)\n",
    "    x = re.sub('與', '여당 ', x)\n",
    "    x = re.sub('野', '야당 ', x)\n",
    "    x = re.sub('黨', '당', x)\n",
    "    x = re.sub('尹', '윤석열', x)\n",
    "    x = re.sub('朴', '박근혜', x)\n",
    "    x = re.sub('文', '문재인', x)\n",
    "    x = re.sub('靑', '청와대', x)\n",
    "    x = re.sub('反', '반대 ', x)\n",
    "    x = re.sub(\"非\", \"아닌 \", x)  # 정치에선 반대랑 같은 의미로 쓰이는데 비청년에서는 반대로 해석하기 좀 그럼\n",
    "    x = re.sub('親', '친', x)\n",
    "    x = re.sub('南', '남한', x)\n",
    "    x = re.sub('軍', '군대', x)\n",
    "    x = re.sub('檢', '검찰', x)\n",
    "    x = re.sub(\"新\", \"새로운 \", x)\n",
    "    x = re.sub(\"弗\", \"달러\", x)\n",
    "    x = re.sub(\"對\", \"대결\", x)\n",
    "    x = re.sub(\"男\", \"남자\", x)\n",
    "    x = re.sub('女', '여자 ', x)\n",
    "    x = re.sub('故', '고인', x)\n",
    "    x = re.sub('前', '이전 ', x)\n",
    "    x = re.sub('株', '주식 ', x)\n",
    "    x = re.sub(\"發\", \"발생\", x)\n",
    "    x = re.sub(\"重\", \"중공업\", x) \n",
    "    x = re.sub('家', '집안', x)\n",
    "    x = re.sub('丁', '정세균', x) \n",
    "    x = re.sub('乙', '을', x)  \n",
    "    x = re.sub('人', '명', x)\n",
    "    x = re.sub('代', '대', x)\n",
    "    x = re.sub('先', '먼저 ', x)\n",
    "    x = re.sub('全', '전체', x)  # '전'세계에서는 조금 어색\n",
    "    x = re.sub('前', '이전', x)\n",
    "    x = re.sub('史', '역사', x)\n",
    "    x = re.sub('和', '화합', x)  \n",
    "    x = re.sub(\"外\", \"외\", x)  # 외 -> 불용어  \n",
    "    x = re.sub(\"大\", \"대학교\", x)\n",
    "    x = re.sub('妻', '부인', x)\n",
    "    x = re.sub(\"子\", \"아들\", x)\n",
    "    x = re.sub(\"孫\", \"손학규\", x) # 성씨 손 -> train에선 다 손학규로 쓰였는데 다른 손씨 (우리금융회장)으로 쓰이면? \n",
    "    x = re.sub('安', '안철수', x) # 안철수로 많이 쓰이지만 but 안(내부) 으로도 쓰임 -> 어떻게하지\n",
    "    x = re.sub('富', '부', x)\n",
    "    x = re.sub('對', '대', x)  # 대결, 맞서다 \n",
    "    x = re.sub('山', '산', x)\n",
    "    x = re.sub('崔', '최순실', x)  \n",
    "    x = re.sub('州', '주', x)  # 외국 지역 단위 주 -> 삭제해도 될 듯?\n",
    "    x = re.sub('市', '도시', x)\n",
    "    x = re.sub('後', '이후', x)\n",
    "    x = re.sub(\"情\", \"정\", x) \n",
    "    x = re.sub('惡', '악', x)\n",
    "    x = re.sub('曺', '조국', x)  # 법무부장관\n",
    "    x = re.sub('月', '월', x)\n",
    "    x = re.sub(\"核\", \"핵\", x)\n",
    "    x = re.sub(\"案\", \"안건\", x)\n",
    "    x = re.sub(\"機\", \"비행기\", x)\n",
    "    x = re.sub(\"氣\", \"기\", x)\n",
    "    x = re.sub('江', '강', x)\n",
    "    x = re.sub('法', '법', x)\n",
    "    # x = re.sub('洪', '홍', x)  # 성씨 -> 머투 홍선근 회장, 홍준표 정치인\n",
    "    x = re.sub('湖', '호수', x)\n",
    "    x = re.sub(\"潘\", \"반기문\", x)\n",
    "    x = re.sub(\"無\", \"없다\", x) \n",
    "    x = re.sub('燈', '등', x) # 등불 -> 삭제? \n",
    "    x = re.sub('父', '아버지', x)\n",
    "    x = re.sub('王', '왕', x)\n",
    "    x = re.sub('現', '현재', x)\n",
    "    x = re.sub('申', '신태용', x) # 축구감독, test에서는 다른 신씨로 쓰이면? \n",
    "    x = re.sub('知', '', x)  # 삭제 (앞에다 음 써놓음)\n",
    "    x = re.sub('硏', '연구소', x)\n",
    "    x = re.sub('社', '회사', x)  # 社告는 삭제해도 될 듯 (속보 같은 것)\n",
    "    x = re.sub('禹', '우병우', x)\n",
    "    x = re.sub(\"秋\", \"추미애\", x)  # test에서 다른 추씨 나오면?\n",
    "    x = re.sub(\"稅\", \"세\", x) # 세금\n",
    "    x = re.sub('臺', '대만', x)\n",
    "    x = re.sub(\"色\", \"색\", x)\n",
    "    x = re.sub(\"萬\", \"인도\", x)\n",
    "    x = re.sub('號', '호', x)  # ooo호\n",
    "    x = re.sub('行', '행', x)\n",
    "    x = re.sub('詩', '시', x)\n",
    "    x = re.sub('賞', '상', x)\n",
    "    x = re.sub('趙', '조', x)  # 조윤선 -> test에서 다른 조씨 쓰이면? \n",
    "    x = re.sub('車', '자동차', x)\n",
    "    x = re.sub('道', '도로', x)\n",
    "    x = re.sub('選', '', x)  # 삭제해도 괜춘\n",
    "    x = re.sub(\"鄭\", \"정진석\", x)  \n",
    "    x = re.sub('金', '김정은', x)  # 김씨가 너무 많음, 김정은으로 젤 많이 쓰인 듯 함\n",
    "    if re.search(\"[가-핳]銀\", x):\n",
    "        x = re.sub(\"銀\", \"은행\", x)\n",
    "    x = re.sub('銀', '은메달', x)\n",
    "    x = re.sub('銅', '동메달', x)\n",
    "    x = re.sub('阿', '아프리카', x)\n",
    "    x = re.sub(\"院\", \"국회 원\", x)  # 국회 원 구성하다\n",
    "    x = re.sub(\"黃\", \"황교안\", x)\n",
    "    x = re.sub(\"黨\", \"당\", x)\n",
    "    x = re.sub(\"蘭\", \"\", x) # 난초 -> 삭제해도 괜춘\n",
    "    x = re.sub(\"盧\", \"노무현\", x)\n",
    "    x = re.sub(\"樂\", \"\", x)  # 락 -> 삭제해도 괜춘\n",
    "    x = re.sub(\"年\", \"연간 \", x)  # 1년\n",
    "    x = re.sub(\"劉\", \"유승민\", x)\n",
    "    # x = re.sub(\"李\", \"이 씨\", x) # 이씨 너무 많음 -> train 4명 다 다른 인물 지칭\n",
    "    x = re.sub(\"社告\", \"\", x)  # 속보같은 단어들\n",
    "    x = re.sub(\"書香萬里\", \"\", x)\n",
    "    x = re.sub(\"破七\", \"\", x) # 포치: 옆에 한글로 써있음\n",
    "    x = re.sub(\"氷魚\", \"\", x)  # 빙어 :옆에 한글로 써있음 \n",
    "    x = re.sub(\"不備\", \"\", x)  # 불비 : 옆에 한글로 써있음\n",
    "    x = re.sub(\"令和\", \"\", x)  # 일본 연호 : 삭제 무방\n",
    "    # x = hanja.translate(x, 'substitution')\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710d1843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16098</th>\n",
       "      <td>16098</td>\n",
       "      <td>安 민생·지역경제·미래가 키워드 劉 먹고사는 문제 해결종합2보</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17178</th>\n",
       "      <td>17178</td>\n",
       "      <td>바른미래 의총 충돌…孫·金 정면돌파 vs 劉·安계 실력행사</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25304</th>\n",
       "      <td>25304</td>\n",
       "      <td>바른미래 대화록 폭로전…劉 뒤에서 조종 vs 새빨간 거짓말종합2보</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30316</th>\n",
       "      <td>30316</td>\n",
       "      <td>유승민 공천여부 내일은 발표될까…劉 일주일째 칩거</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                 title  topic_idx\n",
       "16098  16098    安 민생·지역경제·미래가 키워드 劉 먹고사는 문제 해결종합2보          6\n",
       "17178  17178      바른미래 의총 충돌…孫·金 정면돌파 vs 劉·安계 실력행사          6\n",
       "25304  25304  바른미래 대화록 폭로전…劉 뒤에서 조종 vs 새빨간 거짓말종합2보          6\n",
       "30316  30316           유승민 공천여부 내일은 발표될까…劉 일주일째 칩거          6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"title\"].str.contains('劉')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3e58f",
   "metadata": {},
   "source": [
    "## 동의어 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10c6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 의미를 뜻하는 단어, 줄임말, 외래어 -> 하나의 단어로 통일\n",
    "# 비슷한 의미를 가진 단어들 중 가장 빈도수가 많은 것으로 통일함 \n",
    "# 내 뇌피셜로 해서 부정확(?), 아이폰, 갤럭시 -> 다 폰으로 할 것인지 ? \n",
    "\n",
    "def same_meaning(x):\n",
    "    x = re.sub(\"스마트폰\", \"폰\", x)\n",
    "    x = re.sub(\"휴대폰\", \"폰\", x)\n",
    "    # x = re.sub(\"갤\", \"갤럭시\")  # 갤럭시럭시 이렇게 돼서 취소\n",
    "    x = re.sub(\"lg 30\", \"LG 폰\", x)\n",
    "    x = re.sub('LG G3', 'LG 폰', x) # lg폰으로 ?\n",
    "    x = re.sub(\"V50S\", 'LG 폰', x)\n",
    "    x = re.sub(\"LG V30\", \"LG 폰\", x)\n",
    "    x = re.sub(\"V30\", \"LG 폰\", x)\n",
    "    x = re.sub('갤럭시 노트', '갤럭시', x)\n",
    "    # x = re.sub(\"갤럭시 10\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시노트8\", \"갤럭시\", x)\n",
    "    # x = re.sub(\"갤럭시\")\n",
    "    x = re.sub(\"갤럭시A70\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시A80\", \"갤럭시\", x)\n",
    "    x = re.sub(\"A6\", \"갤럭시\", x)\n",
    "    x = re.sub(\"A8\", \"갤럭시\", x)\n",
    "    x = re.sub(\"A9\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시S8\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤S8\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시S9\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시S10\", \"갤럭시\", x)\n",
    "    x = re.sub(\"갤럭시 탭\", \"갤럭시\", x)\n",
    "    # x = re.sub(\"갤럭시 S10\", \"갤럭시\", x)\n",
    "    # x = re.sub(\"갤럭시 폴드\", \"갤럭시\", x)\n",
    "    x = re.sub(\"S7\", \"갤럭시\", x)\n",
    "    x = re.sub(\"삼성 노트\", \"갤럭시\", x)\n",
    "    x = re.sub(\"XE\", \"아이폰\", x)\n",
    "    x = re.sub(\"아이폰X\", \"아이폰\", x)\n",
    "    x = re.sub(\"6S\", \"아이폰\", x)\n",
    "    # x = re.sub('삼성전자', '삼성', x)\n",
    "    x = re.sub(\"삼성 전자\", \"삼성\", x)\n",
    "    x = re.sub(\"인공 지능\", \"인공지능\", x)\n",
    "    x = re.sub('카카오톡', '카톡', x)\n",
    "    # x = re.sub(\"패드\", \"태블릿\", x)\n",
    "    x = re.sub('지난해', '전년', x)\n",
    "    x = re.sub(\"금융감독원\", \"금감원\", x)\n",
    "    x = re.sub(\"리스크\", \"위험\", x)\n",
    "    x = re.sub('테크', '기술', x)\n",
    "    # x = re.sub(\"베스트\", \"최고\", x)  # 베스트셀러 포함되나? ->  포함됨\n",
    "    x = re.sub(\"한해\", \"연간\", x)\n",
    "    x = re.sub(\"대한민국\", \"한국\", x)\n",
    "    x = re.sub('남한', '한국', x)\n",
    "    x = re.sub(\"온도\", \"기온\", x)\n",
    "    x = re.sub('카카오톡', '카톡', x)\n",
    "    x = re.sub(\"대학교\", \"대학\", x)\n",
    "    x = re.sub('스토어', '매장', x)\n",
    "    x = re.sub(\"점포\", \"매장\", x)\n",
    "    x = re.sub(\"고등학교\", \"고교\", x)\n",
    "    x = re.sub(\"대법원\", \"대법\", x)\n",
    "    x = re.sub('시내버스', '버스', x)\n",
    "    x = re.sub(\"페스티벌\", \"축제\", x)\n",
    "    x = re.sub(\"제주도\", \"제주\", x)\n",
    "    x = re.sub('경기도', '경기', x) # 시합 경기랑 헷갈리려나\n",
    "    x = re.sub(\"강원도\", \"강원\", x)\n",
    "    x = re.sub('전라도', '전라', x)\n",
    "    x = re.sub(\"트렌드\", \"유행\", x)\n",
    "    x = re.sub('서울시', '서울', x)\n",
    "    x = re.sub(\"페북\", \"페이스북\", x)\n",
    "    x = re.sub('군대', '군', x)\n",
    "    x = re.sub(\"AG\", \"아시안게임\", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f989c",
   "metadata": {},
   "source": [
    "## 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4548ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 수가 1글자인 단어만 추출해봄\n",
    "\n",
    "corpus1 = []\n",
    "titles = list(train[\"title\"].values)\n",
    "\n",
    "for t in titles:\n",
    "    for n in m.nouns(t):\n",
    "        if len(n) == 1:\n",
    "            corpus1.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a71617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ㄴ',\n",
       " '丁',\n",
       " '中',\n",
       " '乙',\n",
       " '人',\n",
       " '代',\n",
       " '伊',\n",
       " '佛',\n",
       " '先',\n",
       " '全',\n",
       " '前',\n",
       " '北',\n",
       " '南',\n",
       " '印',\n",
       " '反',\n",
       " '史',\n",
       " '和',\n",
       " '外',\n",
       " '大',\n",
       " '妻',\n",
       " '子',\n",
       " '孫',\n",
       " '安',\n",
       " '富',\n",
       " '對',\n",
       " '尹',\n",
       " '山',\n",
       " '崔',\n",
       " '州',\n",
       " '市',\n",
       " '後',\n",
       " '情',\n",
       " '惡',\n",
       " '文',\n",
       " '日',\n",
       " '書',\n",
       " '曺',\n",
       " '月',\n",
       " '朴',\n",
       " '株',\n",
       " '核',\n",
       " '案',\n",
       " '機',\n",
       " '比',\n",
       " '氣',\n",
       " '江',\n",
       " '法',\n",
       " '洪',\n",
       " '湖',\n",
       " '潘',\n",
       " '無',\n",
       " '燈',\n",
       " '父',\n",
       " '獨',\n",
       " '王',\n",
       " '現',\n",
       " '申',\n",
       " '男',\n",
       " '知',\n",
       " '破',\n",
       " '硏',\n",
       " '社',\n",
       " '禹',\n",
       " '秋',\n",
       " '稅',\n",
       " '美',\n",
       " '臺',\n",
       " '與',\n",
       " '色',\n",
       " '英',\n",
       " '萬',\n",
       " '號',\n",
       " '行',\n",
       " '詩',\n",
       " '賞',\n",
       " '趙',\n",
       " '車',\n",
       " '軍',\n",
       " '道',\n",
       " '選',\n",
       " '鄭',\n",
       " '里',\n",
       " '野',\n",
       " '金',\n",
       " '銀',\n",
       " '銅',\n",
       " '阿',\n",
       " '院',\n",
       " '靑',\n",
       " '韓',\n",
       " '香',\n",
       " '魚',\n",
       " '黃',\n",
       " '黨',\n",
       " '각',\n",
       " '간',\n",
       " '갈',\n",
       " '감',\n",
       " '갑',\n",
       " '값',\n",
       " '강',\n",
       " '개',\n",
       " '객',\n",
       " '갤',\n",
       " '갭',\n",
       " '갱',\n",
       " '거',\n",
       " '건',\n",
       " '걸',\n",
       " '검',\n",
       " '겁',\n",
       " '것',\n",
       " '겉',\n",
       " '게',\n",
       " '겔',\n",
       " '겨',\n",
       " '견',\n",
       " '결',\n",
       " '겸',\n",
       " '경',\n",
       " '곁',\n",
       " '계',\n",
       " '고',\n",
       " '곡',\n",
       " '골',\n",
       " '곰',\n",
       " '곳',\n",
       " '공',\n",
       " '곶',\n",
       " '과',\n",
       " '곽',\n",
       " '관',\n",
       " '괌',\n",
       " '광',\n",
       " '교',\n",
       " '구',\n",
       " '국',\n",
       " '군',\n",
       " '굳',\n",
       " '굴',\n",
       " '굿',\n",
       " '궁',\n",
       " '궈',\n",
       " '권',\n",
       " '궤',\n",
       " '귀',\n",
       " '규',\n",
       " '균',\n",
       " '그',\n",
       " '극',\n",
       " '근',\n",
       " '글',\n",
       " '금',\n",
       " '급',\n",
       " '기',\n",
       " '길',\n",
       " '김',\n",
       " '깨',\n",
       " '껌',\n",
       " '꼴',\n",
       " '꽃',\n",
       " '꿀',\n",
       " '꿈',\n",
       " '끈',\n",
       " '끌',\n",
       " '끗',\n",
       " '끝',\n",
       " '끼',\n",
       " '나',\n",
       " '낙',\n",
       " '난',\n",
       " '날',\n",
       " '남',\n",
       " '납',\n",
       " '낫',\n",
       " '낭',\n",
       " '낮',\n",
       " '내',\n",
       " '낸',\n",
       " '냥',\n",
       " '너',\n",
       " '넉',\n",
       " '널',\n",
       " '넛',\n",
       " '네',\n",
       " '넥',\n",
       " '넷',\n",
       " '녀',\n",
       " '년',\n",
       " '노',\n",
       " '녹',\n",
       " '논',\n",
       " '놈',\n",
       " '농',\n",
       " '놓',\n",
       " '뇌',\n",
       " '누',\n",
       " '눈',\n",
       " '뉴',\n",
       " '능',\n",
       " '늪',\n",
       " '니',\n",
       " '닉',\n",
       " '님',\n",
       " '닝',\n",
       " '다',\n",
       " '닥',\n",
       " '닦',\n",
       " '단',\n",
       " '달',\n",
       " '닭',\n",
       " '담',\n",
       " '답',\n",
       " '닷',\n",
       " '당',\n",
       " '닻',\n",
       " '대',\n",
       " '댁',\n",
       " '댐',\n",
       " '댓',\n",
       " '덕',\n",
       " '덤',\n",
       " '덫',\n",
       " '데',\n",
       " '덴',\n",
       " '델',\n",
       " '뎅',\n",
       " '뎐',\n",
       " '뎡',\n",
       " '도',\n",
       " '독',\n",
       " '돈',\n",
       " '돌',\n",
       " '돔',\n",
       " '동',\n",
       " '둑',\n",
       " '둘',\n",
       " '둥',\n",
       " '뒤',\n",
       " '듀',\n",
       " '드',\n",
       " '득',\n",
       " '듯',\n",
       " '등',\n",
       " '디',\n",
       " '딕',\n",
       " '딜',\n",
       " '딥',\n",
       " '딩',\n",
       " '딸',\n",
       " '땀',\n",
       " '땅',\n",
       " '때',\n",
       " '땐',\n",
       " '떡',\n",
       " '떨',\n",
       " '떼',\n",
       " '똥',\n",
       " '뚜',\n",
       " '뚝',\n",
       " '뛰',\n",
       " '뜰',\n",
       " '뜸',\n",
       " '뜻',\n",
       " '락',\n",
       " '란',\n",
       " '랄',\n",
       " '람',\n",
       " '랍',\n",
       " '래',\n",
       " '랙',\n",
       " '랜',\n",
       " '램',\n",
       " '랩',\n",
       " '랭',\n",
       " '량',\n",
       " '럭',\n",
       " '런',\n",
       " '럼',\n",
       " '럽',\n",
       " '레',\n",
       " '렉',\n",
       " '렌',\n",
       " '렛',\n",
       " '려',\n",
       " '련',\n",
       " '렴',\n",
       " '렸',\n",
       " '령',\n",
       " '례',\n",
       " '록',\n",
       " '롤',\n",
       " '롬',\n",
       " '롯',\n",
       " '롱',\n",
       " '료',\n",
       " '룡',\n",
       " '루',\n",
       " '룬',\n",
       " '룰',\n",
       " '룸',\n",
       " '룹',\n",
       " '룽',\n",
       " '뤄',\n",
       " '뤼',\n",
       " '뤽',\n",
       " '류',\n",
       " '륙',\n",
       " '륭',\n",
       " '르',\n",
       " '륵',\n",
       " '릉',\n",
       " '리',\n",
       " '릭',\n",
       " '린',\n",
       " '릴',\n",
       " '림',\n",
       " '립',\n",
       " '릿',\n",
       " '링',\n",
       " '마',\n",
       " '막',\n",
       " '만',\n",
       " '말',\n",
       " '맘',\n",
       " '맛',\n",
       " '망',\n",
       " '맞',\n",
       " '맡',\n",
       " '매',\n",
       " '맥',\n",
       " '맨',\n",
       " '맴',\n",
       " '맵',\n",
       " '맷',\n",
       " '맹',\n",
       " '머',\n",
       " '먹',\n",
       " '멀',\n",
       " '멋',\n",
       " '멍',\n",
       " '메',\n",
       " '멘',\n",
       " '멜',\n",
       " '멤',\n",
       " '면',\n",
       " '명',\n",
       " '모',\n",
       " '목',\n",
       " '몫',\n",
       " '몬',\n",
       " '몰',\n",
       " '몸',\n",
       " '몹',\n",
       " '못',\n",
       " '몽',\n",
       " '묘',\n",
       " '무',\n",
       " '묵',\n",
       " '문',\n",
       " '물',\n",
       " '뭐',\n",
       " '뭘',\n",
       " '뮤',\n",
       " '뮬',\n",
       " '믈',\n",
       " '미',\n",
       " '믹',\n",
       " '민',\n",
       " '밀',\n",
       " '밋',\n",
       " '밍',\n",
       " '밑',\n",
       " '바',\n",
       " '박',\n",
       " '밖',\n",
       " '반',\n",
       " '발',\n",
       " '밤',\n",
       " '밥',\n",
       " '방',\n",
       " '밭',\n",
       " '배',\n",
       " '백',\n",
       " '밴',\n",
       " '밸',\n",
       " '뱀',\n",
       " '버',\n",
       " '번',\n",
       " '벌',\n",
       " '범',\n",
       " '법',\n",
       " '벗',\n",
       " '베',\n",
       " '벡',\n",
       " '벤',\n",
       " '벨',\n",
       " '벼',\n",
       " '벽',\n",
       " '변',\n",
       " '별',\n",
       " '병',\n",
       " '보',\n",
       " '복',\n",
       " '본',\n",
       " '볼',\n",
       " '봄',\n",
       " '봇',\n",
       " '봉',\n",
       " '부',\n",
       " '북',\n",
       " '분',\n",
       " '불',\n",
       " '붐',\n",
       " '붓',\n",
       " '뷔',\n",
       " '뷰',\n",
       " '브',\n",
       " '블',\n",
       " '비',\n",
       " '빅',\n",
       " '빈',\n",
       " '빌',\n",
       " '빔',\n",
       " '빗',\n",
       " '빙',\n",
       " '빚',\n",
       " '빛',\n",
       " '빠',\n",
       " '빵',\n",
       " '뻔',\n",
       " '뼈',\n",
       " '뼘',\n",
       " '뽕',\n",
       " '뿐',\n",
       " '뿔',\n",
       " '사',\n",
       " '삭',\n",
       " '산',\n",
       " '살',\n",
       " '삶',\n",
       " '삼',\n",
       " '삽',\n",
       " '삿',\n",
       " '상',\n",
       " '새',\n",
       " '색',\n",
       " '샘',\n",
       " '생',\n",
       " '샤',\n",
       " '샬',\n",
       " '샵',\n",
       " '샷',\n",
       " '샹',\n",
       " '서',\n",
       " '석',\n",
       " '선',\n",
       " '설',\n",
       " '섬',\n",
       " '섭',\n",
       " '성',\n",
       " '세',\n",
       " '센',\n",
       " '셀',\n",
       " '셈',\n",
       " '셉',\n",
       " '셋',\n",
       " '셍',\n",
       " '셔',\n",
       " '션',\n",
       " '셜',\n",
       " '셰',\n",
       " '셴',\n",
       " '셸',\n",
       " '소',\n",
       " '속',\n",
       " '손',\n",
       " '솔',\n",
       " '송',\n",
       " '솥',\n",
       " '쇄',\n",
       " '쇠',\n",
       " '쇼',\n",
       " '숀',\n",
       " '숄',\n",
       " '숍',\n",
       " '수',\n",
       " '순',\n",
       " '술',\n",
       " '숨',\n",
       " '숭',\n",
       " '숲',\n",
       " '슈',\n",
       " '슛',\n",
       " '스',\n",
       " '승',\n",
       " '시',\n",
       " '식',\n",
       " '신',\n",
       " '실',\n",
       " '심',\n",
       " '십',\n",
       " '싱',\n",
       " '싹',\n",
       " '쌀',\n",
       " '쌍',\n",
       " '썬',\n",
       " '썸',\n",
       " '쏘',\n",
       " '쏙',\n",
       " '쏠',\n",
       " '쐐',\n",
       " '쑥',\n",
       " '쑹',\n",
       " '쓰',\n",
       " '씨',\n",
       " '씬',\n",
       " '악',\n",
       " '안',\n",
       " '알',\n",
       " '앓',\n",
       " '암',\n",
       " '앙',\n",
       " '앞',\n",
       " '애',\n",
       " '액',\n",
       " '앤',\n",
       " '앰',\n",
       " '앱',\n",
       " '약',\n",
       " '얀',\n",
       " '양',\n",
       " '억',\n",
       " '언',\n",
       " '얼',\n",
       " '엄',\n",
       " '업',\n",
       " '엇',\n",
       " '엑',\n",
       " '엔',\n",
       " '엘',\n",
       " '엠',\n",
       " '여',\n",
       " '역',\n",
       " '연',\n",
       " '열',\n",
       " '영',\n",
       " '옆',\n",
       " '예',\n",
       " '옐',\n",
       " '오',\n",
       " '옥',\n",
       " '올',\n",
       " '옴',\n",
       " '옷',\n",
       " '옻',\n",
       " '와',\n",
       " '완',\n",
       " '왓',\n",
       " '왕',\n",
       " '외',\n",
       " '욕',\n",
       " '용',\n",
       " '우',\n",
       " '욱',\n",
       " '운',\n",
       " '울',\n",
       " '움',\n",
       " '웅',\n",
       " '워',\n",
       " '원',\n",
       " '월',\n",
       " '웜',\n",
       " '웡',\n",
       " '웨',\n",
       " '웰',\n",
       " '웹',\n",
       " '위',\n",
       " '윅',\n",
       " '윈',\n",
       " '윌',\n",
       " '윗',\n",
       " '윙',\n",
       " '유',\n",
       " '육',\n",
       " '윤',\n",
       " '윳',\n",
       " '융',\n",
       " '음',\n",
       " '읍',\n",
       " '의',\n",
       " '이',\n",
       " '익',\n",
       " '인',\n",
       " '일',\n",
       " '임',\n",
       " '입',\n",
       " '잇',\n",
       " '잉',\n",
       " '잎',\n",
       " '자',\n",
       " '작',\n",
       " '잔',\n",
       " '잠',\n",
       " '장',\n",
       " '재',\n",
       " '잭',\n",
       " '잼',\n",
       " '저',\n",
       " '적',\n",
       " '전',\n",
       " '절',\n",
       " '점',\n",
       " '정',\n",
       " '제',\n",
       " '젠',\n",
       " '젤',\n",
       " '젬',\n",
       " '젭',\n",
       " '젯',\n",
       " '져',\n",
       " '조',\n",
       " '족',\n",
       " '존',\n",
       " '좁',\n",
       " '종',\n",
       " '좌',\n",
       " '죄',\n",
       " '죠',\n",
       " '주',\n",
       " '죽',\n",
       " '준',\n",
       " '줄',\n",
       " '줌',\n",
       " '중',\n",
       " '쥐',\n",
       " '쥬',\n",
       " '쥴',\n",
       " '즈',\n",
       " '즘',\n",
       " '증',\n",
       " '지',\n",
       " '직',\n",
       " '진',\n",
       " '질',\n",
       " '짐',\n",
       " '집',\n",
       " '짓',\n",
       " '징',\n",
       " '짝',\n",
       " '짱',\n",
       " '쪽',\n",
       " '쯤',\n",
       " '찜',\n",
       " '차',\n",
       " '찬',\n",
       " '참',\n",
       " '창',\n",
       " '채',\n",
       " '책',\n",
       " '챗',\n",
       " '챠',\n",
       " '처',\n",
       " '척',\n",
       " '천',\n",
       " '철',\n",
       " '첩',\n",
       " '청',\n",
       " '체',\n",
       " '초',\n",
       " '촌',\n",
       " '총',\n",
       " '최',\n",
       " '추',\n",
       " '축',\n",
       " '춘',\n",
       " '춤',\n",
       " '충',\n",
       " '취',\n",
       " '츄',\n",
       " '츠',\n",
       " '측',\n",
       " '층',\n",
       " '치',\n",
       " '친',\n",
       " '칠',\n",
       " '침',\n",
       " '칩',\n",
       " '칭',\n",
       " '카',\n",
       " '칸',\n",
       " '칼',\n",
       " '캅',\n",
       " '캉',\n",
       " '캔',\n",
       " '캠',\n",
       " '캡',\n",
       " '캣',\n",
       " '커',\n",
       " '컨',\n",
       " '컴',\n",
       " '컵',\n",
       " '컷',\n",
       " '케',\n",
       " '켄',\n",
       " '켈',\n",
       " '켐',\n",
       " '켠',\n",
       " '코',\n",
       " '콕',\n",
       " '콘',\n",
       " '콜',\n",
       " '콩',\n",
       " '콰',\n",
       " '쿠',\n",
       " '쿡',\n",
       " '쿤',\n",
       " '쿨',\n",
       " '쿼',\n",
       " '퀸',\n",
       " '큐',\n",
       " '크',\n",
       " '클',\n",
       " '키',\n",
       " '킥',\n",
       " '킨',\n",
       " '킴',\n",
       " '킷',\n",
       " '킹',\n",
       " '타',\n",
       " '탄',\n",
       " '탈',\n",
       " '탐',\n",
       " '탑',\n",
       " '탓',\n",
       " '탕',\n",
       " '태',\n",
       " '택',\n",
       " '탬',\n",
       " '탭',\n",
       " '탱',\n",
       " '터',\n",
       " '턴',\n",
       " '털',\n",
       " '텀',\n",
       " '테',\n",
       " '텍',\n",
       " '텐',\n",
       " '텔',\n",
       " '토',\n",
       " '톡',\n",
       " '톤',\n",
       " '톰',\n",
       " '톱',\n",
       " '톳',\n",
       " '통',\n",
       " '퇴',\n",
       " '투',\n",
       " '툰',\n",
       " '툴',\n",
       " '툼',\n",
       " '튀',\n",
       " '튜',\n",
       " '트',\n",
       " '틀',\n",
       " '틈',\n",
       " '티',\n",
       " '틱',\n",
       " '틴',\n",
       " '틸',\n",
       " '팀',\n",
       " '팁',\n",
       " '파',\n",
       " '판',\n",
       " '팔',\n",
       " '팜',\n",
       " '팝',\n",
       " '팟',\n",
       " '패',\n",
       " '팩',\n",
       " '팬',\n",
       " '팸',\n",
       " '팹',\n",
       " '팻',\n",
       " '펀',\n",
       " '펄',\n",
       " '펌',\n",
       " '펙',\n",
       " '펜',\n",
       " '펠',\n",
       " '펩',\n",
       " '펫',\n",
       " '펭',\n",
       " '편',\n",
       " '평',\n",
       " '폐',\n",
       " '포',\n",
       " '폭',\n",
       " '폰',\n",
       " '폴',\n",
       " '퐁',\n",
       " '푄',\n",
       " '표',\n",
       " '푸',\n",
       " '푼',\n",
       " '풀',\n",
       " '품',\n",
       " '퓨',\n",
       " '플',\n",
       " '피',\n",
       " '픽',\n",
       " '핀',\n",
       " '필',\n",
       " '핏',\n",
       " '핑',\n",
       " '학',\n",
       " '한',\n",
       " '할',\n",
       " '함',\n",
       " '핫',\n",
       " '항',\n",
       " '해',\n",
       " '핵',\n",
       " '햄',\n",
       " '햅',\n",
       " '행',\n",
       " '향',\n",
       " '헨',\n",
       " '헬',\n",
       " '혁',\n",
       " '협',\n",
       " '형',\n",
       " '호',\n",
       " '혹',\n",
       " '혼',\n",
       " '홀',\n",
       " '홈',\n",
       " '홉',\n",
       " '홍',\n",
       " '화',\n",
       " '환',\n",
       " '활',\n",
       " '황',\n",
       " '회',\n",
       " '횡',\n",
       " '효',\n",
       " '후',\n",
       " '훈',\n",
       " '훙',\n",
       " '휘',\n",
       " '휠',\n",
       " '휴',\n",
       " '흑',\n",
       " '흙',\n",
       " '흥',\n",
       " '희',\n",
       " '힌',\n",
       " '힐',\n",
       " '힘',\n",
       " '힙',\n",
       " '金',\n",
       " '蘭',\n",
       " '盧',\n",
       " '樂',\n",
       " '不',\n",
       " '女',\n",
       " '年',\n",
       " '劉',\n",
       " '李'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list에서 set으로 -> 중복되는 단어 제거\n",
    "corpus1 = set(corpus1)\n",
    "corpus1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc6b7e",
   "metadata": {},
   "source": [
    "korean stopwords reference : https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c719e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577a0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 : 의미분석에 있어서 불필요한 단어들의 집합 (서술어, 조사)\n",
    "# 1 글자더라도 의미있는 단어가 있기에 그렇게 생각되는 1글자는 리스트에서 뺌\n",
    "# 어떻게 쓰였는지 모르겠는 1글자들이 너무 많음,,,,,,,,,\n",
    "# 단위는 제거 안함 -> 기준 너무 애매모호함,,,,,,,,,,,\n",
    "\n",
    "def stopwords(title):\n",
    "    stopwords = [\"만\", \"만큼\", \"성큼\", \"그만큼\", \"에\" \"등\", \"듯\", \"외\", \"의\", \"정말\", \"것\", \"같다\", \"관한\",\n",
    "                 \"그래서\", \"그렇게\", \"그렇다\", \"그렇지\", \"때\", \"때문에\", \"때문\", \"를\", \"마저\", \"부터\", \"어느\",\n",
    "                 \"어떻게\", \"에서\", \"으로서\", \"와\", \"과\", \"할때\", \"할 때\", \"혹시\", \"혹시나\", \"해요\", \"했어요\", \"이다\",\n",
    "                 \"이\", \"가\", \"을\", \"를\", \"은\", \"는\", \"에게\", \"되다\", \"하다\", \"하다가\", \"뿐\", \"대하여\", \"관하여\",\n",
    "                 \"과연\", \"그래도\", \"그리고\", \"및\", \"결국\", \"좀\", \"자마자\", \"곧\", \"로부터\", \"동안\", \"해야\", \"그\", \n",
    "                 \"저\", \"우선\"]\n",
    "    for t in title:\n",
    "        word_tokens = word_tokenize(t)\n",
    "        result = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stopwords:\n",
    "                result.append(w)\n",
    "                n_title = \" \".join(result)\n",
    "        return n_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bf0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(title):\n",
    "    stopwords = [\"만\", \"만큼\", \"성큼\", \"그만큼\", \"에\" \"등\", \"듯\", \"외\", \"의\", \"정말\", \"것\", \"같다\", \"관한\",\n",
    "                 \"그래서\", \"그렇게\", \"그렇다\", \"그렇지\", \"때\", \"때문에\", \"때문\", \"를\", \"마저\", \"부터\", \"어느\",\n",
    "                 \"어떻게\", \"에서\", \"으로서\", \"와\", \"과\", \"할때\", \"할 때\", \"혹시\", \"혹시나\", \"해요\", \"했어요\", \"이다\",\n",
    "                 \"이\", \"가\", \"을\", \"를\", \"은\", \"는\", \"에게\", \"되다\", \"하다\", \"하다가\", \"뿐\", \"대하여\", \"관하여\",\n",
    "                 \"과연\", \"그래도\", \"그리고\", \"및\", \"결국\", \"좀\", \"자마자\", \"곧\", \"로부터\", \"동안\", \"해야\", \"그\", \n",
    "                 \"저\", \"우선\", \"앗\", \"슝\"]\n",
    "    for t in title:\n",
    "        word_tokens = word_tokenize(t)\n",
    "        result = []\n",
    "        for w in word_tokens:\n",
    "            if w in stopwords:\n",
    "                title.replace(w, \" \")\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6b876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'트럼프 미일동맹 강조했지만…무역적자 못 믿을 만큼 커 압박'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords(train[\"title\"][3442])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9445c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(title):\n",
    "    stop_words = \"만 지만 만큼 성큼 그만큼 이만큼 에 등 듯 외 의 정말 것 같다 관한 그래서 그렇게 그렇다 그렇지 때 때문에 때문 를 마저 부터 어느 어떻게 에서 으로서 와 과 할때 할 때 혹시 혹시나 해요 했어요 이다 이 가 을 를 은 는 에게 되다 하다 하다가 뿐 대하여 관하여 과연 그래도 그리고 및 결국 좀 자마자 곧 로부터 동안 해야 그 저 우선\"\n",
    "    stop_words = stop_words.split(' ')\n",
    "    \n",
    "    word_tokens = word_tokenize(title)\n",
    "    result = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            result.append(w)\n",
    "            n_title = \" \".join(result)\n",
    "    return n_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af25350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(title):\n",
    "    stop_words = \"만 지만 만큼 성큼 그만큼 이만큼 에 등 듯 외 의 정말 것 같다 관한 그래서 그렇게 그렇다 그렇지 때 때문에 때문 를 마저 부터 어느 어떻게 에서 으로서 와 과 할때 할 때 혹시 혹시나 해요 했어요 이다 이 가 을 를 은 는 에게 되다 하다 하다가 뿐 대하여 관하여 과연 그래도 그리고 및 결국 좀 자마자 곧 로부터 동안 해야 그 저 우선\"\n",
    "    stop_words = stop_words.split(' ')\n",
    "    for t in title:\n",
    "        word_tokens = word_tokenize(title)\n",
    "        result = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                result.append(w)\n",
    "                n_title = \" \".join(result)\n",
    "        return n_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45eae369",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6376/3426880467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6376/4275460459.py\u001b[0m in \u001b[0;36mstopwords\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \"\"\"\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \"\"\"\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \"\"\"\n\u001b[0;32m   1364\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "stopwords(train[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94720174",
   "metadata": {},
   "source": [
    "stopwords 토큰화가 잘 안됨 -> 해결 못하고 코드공유에서도 딱히 불용어 제거 안했길래 안하고 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595c0a7",
   "metadata": {},
   "source": [
    "## 불필요한 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "958ae979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 제목에서 \"속보\" 같은 뉴스에서 의미없는 단어들 \n",
    "\n",
    "def word_delete(title): \n",
    "    delete_words = [\"1보\", \"2보\", \"3보\", \"4보\", \"5보\", \"속보\", \"사진\", \"게시판\", \"사진\", \"게시판\", \"주말 N\", \"QA\", \"그래픽\", \"신간\", \n",
    "                    \"위클리\", \"주간 화제의 뉴스\", \"카드뉴스\", \"팩트체크\"]\n",
    "\n",
    "    for i in delete_words:\n",
    "        if title.endswith(i) or title.startswith(i):\n",
    "            title = title.replace(i, \"\")\n",
    "    if title.endswith(\"종합\"):\n",
    "            title = title[:-2]\n",
    "    title = title.upper() \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6a5f9",
   "metadata": {},
   "source": [
    "## 불필요한 텍스트 제거   \n",
    "references : https://hamait.tistory.com/342"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449b1cb",
   "metadata": {},
   "source": [
    "**정규 표현식 : 특정한 조건의 문자를 검색, 추출, 치환하는 과정을 간편하게 처리할 수 있는 수단**  \n",
    "^x : x 문자로 시작됨을 의미  \n",
    "x$ : x 문자로 종료됨을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b401b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(title):\n",
    "    # 한글 (자음, 모음), 영문(소문자, 대문자), 숫자만 남김, 중복생성된 공백 삭제\n",
    "    title_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\\\s.%]\", \" \", title)\n",
    "    title_clean = re.sub(\" +\", \" \", title_clean)\n",
    "    # 문자열 시작과 끝에 있는 공백 제거\n",
    "    title_clean = title_clean.strip()\n",
    "    return title_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d6af1",
   "metadata": {},
   "source": [
    "## 최종 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94bff4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_eda(title):\n",
    "    title = chinese_etc(title)\n",
    "    title = same_meaning(title)\n",
    "    # title = stopwords(title)\n",
    "    title = word_delete(title)\n",
    "    title = clean_text(title)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ae9837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"title\"] = train[\"title\"].apply(word_eda)\n",
    "test[\"title\"] = test['title'].apply(word_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde3707",
   "metadata": {},
   "source": [
    "+주제별 LDA 토픽모델링 (?) : https://happy-obok.tistory.com/5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a435e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>아시안게임 만리장성 맞서는 농구 단일팀 이문규 감독...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>아시안게임 경기장 잔디 점검한 김학범 중동팀에 유리한...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>베트남 언론 아시안게임 첫 4강 박항서 매직 뉴스로 도배</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>아시안게임 여자축구 전가을 황금세대 책임감으로 새 역...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>275</td>\n",
       "      <td>아시안게임 동점골 이민아 선수들 속상해해 태극 낭자...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42184</th>\n",
       "      <td>42184</td>\n",
       "      <td>아시안게임 이만수 부회장 라오스 1승하면 상의 탈의 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42841</th>\n",
       "      <td>42841</td>\n",
       "      <td>아시안게임 여자축구대표팀 최예슬 부상으로 이은미 선발</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42881</th>\n",
       "      <td>42881</td>\n",
       "      <td>아시안게임 금빛 지휘 김학범 성숙해진 손흥민 업그레이드된 황의...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45016</th>\n",
       "      <td>45016</td>\n",
       "      <td>아시안게임 야구대표팀 14안타에 잔루 13개 응집력은 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45587</th>\n",
       "      <td>45587</td>\n",
       "      <td>후보는 4명 선동열호 아시안게임 아마추어 선수 명맥이을까</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                  title  topic_idx\n",
       "77        77        아시안게임 만리장성 맞서는 농구 단일팀 이문규 감독...          5\n",
       "126      126       아시안게임 경기장 잔디 점검한 김학범 중동팀에 유리한...          5\n",
       "155      155        베트남 언론 아시안게임 첫 4강 박항서 매직 뉴스로 도배          4\n",
       "156      156       아시안게임 여자축구 전가을 황금세대 책임감으로 새 역...          5\n",
       "275      275        아시안게임 동점골 이민아 선수들 속상해해 태극 낭자...          5\n",
       "...      ...                                    ...        ...\n",
       "42184  42184       아시안게임 이만수 부회장 라오스 1승하면 상의 탈의 ...          5\n",
       "42841  42841          아시안게임 여자축구대표팀 최예슬 부상으로 이은미 선발          5\n",
       "42881  42881  아시안게임 금빛 지휘 김학범 성숙해진 손흥민 업그레이드된 황의...          5\n",
       "45016  45016      아시안게임 야구대표팀 14안타에 잔루 13개 응집력은 ...          5\n",
       "45587  45587        후보는 4명 선동열호 아시안게임 아마추어 선수 명맥이을까          5\n",
       "\n",
       "[295 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"title\"].str.contains('아시안게임')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c66e7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인천에서핀란드 항공기 결항 휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>실리콘밸리 넘어서겠다 구글 15조원 들여 미국전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NYT 클린턴 측근한국기업 특수관계 조명 공과 사 맞물려</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45649</th>\n",
       "      <td>45649</td>\n",
       "      <td>KB금융 미국 IB 스티펠과 제휴 선진국 시장 공략</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45650</th>\n",
       "      <td>45650</td>\n",
       "      <td>서울교육청 신종코로나 확산에 개학 연기 휴업 검토</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>45651</td>\n",
       "      <td>키움증권 2020 키움 영웅전 실전투자대회</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45652</th>\n",
       "      <td>45652</td>\n",
       "      <td>답변하는 배기동 국립중앙박물관장</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45653</th>\n",
       "      <td>45653</td>\n",
       "      <td>2020 한국인터넷기자상 시상식 내달 1일 개최 특별상 김성후</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45654 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                               title  topic_idx\n",
       "0          0           인천에서핀란드 항공기 결항 휴가철 여행객 분통          4\n",
       "1          1     실리콘밸리 넘어서겠다 구글 15조원 들여 미국전역 거점화          4\n",
       "2          2      이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3          3     NYT 클린턴 측근한국기업 특수관계 조명 공과 사 맞물려          4\n",
       "4          4           시진핑 트럼프에 중미 무역협상 조속 타결 희망          4\n",
       "...      ...                                 ...        ...\n",
       "45649  45649        KB금융 미국 IB 스티펠과 제휴 선진국 시장 공략          1\n",
       "45650  45650         서울교육청 신종코로나 확산에 개학 연기 휴업 검토          2\n",
       "45651  45651             키움증권 2020 키움 영웅전 실전투자대회          1\n",
       "45652  45652                   답변하는 배기동 국립중앙박물관장          2\n",
       "45653  45653  2020 한국인터넷기자상 시상식 내달 1일 개최 특별상 김성후          2\n",
       "\n",
       "[45654 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209a947",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e93923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ed4fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE : 오버샘플링\n",
    "titles_t = train.title.to_numpy().reshape(-1, 1)\n",
    "labels_t = train.topic_idx.to_numpy().reshape(-1, 1)\n",
    "oversample = RandomOverSampler()\n",
    "X_oversample, y_oversample = oversample.fit_resample(titles_t, labels_t)\n",
    "train = pd.DataFrame({\"title\": X_oversample.reshape(-1), 'topic_idx': y_oversample.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aabd91d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>인천에서핀란드 항공기 결항 휴가철 여행객 분통</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>실리콘밸리 넘어서겠다 구글 15조원 들여 미국전역 거점화</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYT 클린턴 측근한국기업 특수관계 조명 공과 사 맞물려</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>시진핑 트럼프에 중미 무역협상 조속 타결 희망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53398</th>\n",
       "      <td>국토위 태풍상륙날 국감장 떠난 이강래 행적 놓고 여야 공방</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53399</th>\n",
       "      <td>김정은 신년사로 미리 본 북한7차 당대회 역사적 분수령</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53400</th>\n",
       "      <td>박근혜대통령 또 북한주민에 메시지 김정은주민 분리전략 본격화</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53401</th>\n",
       "      <td>김관영 내일 탈당 이번주 더민주 이탈 현역 8명 전후</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53402</th>\n",
       "      <td>국감 내일 끝 사립유치원 비리 고용세습 해법 고민 끌어냈다</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53403 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  topic_idx\n",
       "0              인천에서핀란드 항공기 결항 휴가철 여행객 분통          4\n",
       "1        실리콘밸리 넘어서겠다 구글 15조원 들여 미국전역 거점화          4\n",
       "2         이란 외무 긴장완화 해결책은 미국이 경제전쟁 멈추는 것          4\n",
       "3        NYT 클린턴 측근한국기업 특수관계 조명 공과 사 맞물려          4\n",
       "4              시진핑 트럼프에 중미 무역협상 조속 타결 희망          4\n",
       "...                                  ...        ...\n",
       "53398   국토위 태풍상륙날 국감장 떠난 이강래 행적 놓고 여야 공방          6\n",
       "53399     김정은 신년사로 미리 본 북한7차 당대회 역사적 분수령          6\n",
       "53400  박근혜대통령 또 북한주민에 메시지 김정은주민 분리전략 본격화          6\n",
       "53401      김관영 내일 탈당 이번주 더민주 이탈 현역 8명 전후          6\n",
       "53402   국감 내일 끝 사립유치원 비리 고용세습 해법 고민 끌어냈다          6\n",
       "\n",
       "[53403 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "519591c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "train.to_csv(\"C:/Users/kimch/Desktop/NLP_news_topic_classification/preprocessed_train.csv\")\n",
    "test.to_csv(\"C:/Users/kimch/Desktop/NLP_news_topic_classification/preprocessed_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
